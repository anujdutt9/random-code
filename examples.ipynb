{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227865bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from src.data.loader import load_task_dataset\n",
    "from src.data.contrastive_dataset import ContrastiveDatasetConstructor\n",
    "from src.data.evaluation_dataset import EvaluationDatasetConstructor\n",
    "from src.steering.cache_steering import extract_steering_kv, generate_with_cache_steering\n",
    "from src.steering.config import SteeringConfig\n",
    "from src.evaluation.evaluator import Evaluator\n",
    "from src.utils.constants import Tasks\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from dotenv import load_dotenv # Load the HF_TOKEN from .env file if needed\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c29cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "checkpoint = 'HuggingFaceTB/SmolLM2-360M-Instruct'                  # Small model that is fast to run on CPU\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a971e7",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook consists of two parts:\n",
    "\n",
    "Part 1 shows how to use functions from our codebase (NOTE these functions are implemented for the conveniece of running experiments):\n",
    "- [Contrastive Data](#contrastive-data)\n",
    "- [Extracting and applying vectors](#extracting-and-applying-vectors)\n",
    "- [Running experiments with Evaluator class](#running-experiments-with-evaluator-class)\n",
    "\n",
    "\n",
    "Part 2 shows how to use cache steering with **only PyTorch + Transformers**:\n",
    "- [Vector Extraction](#vector-extraction)\n",
    "- [Generation](#generate-with-cache-steering)\n",
    "- [Style Transfer Example](#style-transfer-example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a324a1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a6b8e8",
   "metadata": {},
   "source": [
    "# Running experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0ccbd8",
   "metadata": {},
   "source": [
    "## Contrastive Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c777cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocss ARC-challenge dataset\n",
    "task = Tasks.arc_oai\n",
    "subtask = \"oai\"                         # Subtask to use, can be \"oai\" (general GPT-4o generated traces), \"stepwise\" (Stepwise traces), \"causal_chain\", \"strategy_execution\", or \"analogical reasoning\"\n",
    "dataset = load_task_dataset(Tasks.arc_oai)\n",
    "\n",
    "config = SteeringConfig(\n",
    "    n_contrastive_samples=10,           # Number of contrastive samples to generate\n",
    "    num_fewshot_examples=5,             # Number of few-shot examples to use in each sample\n",
    "    tokenizer=tokenizer,\n",
    "    add_generation_prompt=True,         # Whether to add generation prompt when using chat template\n",
    ")\n",
    "\n",
    "constructor = ContrastiveDatasetConstructor(\n",
    "    dataset[\"train\"],\n",
    "    config,\n",
    "    task=task,\n",
    ")\n",
    "\n",
    "eval_constructor = EvaluationDatasetConstructor(\n",
    "    dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    n=2,                                # Number of samples to use for evaluation\n",
    "    num_fewshot_prompt=0,               # Number of few-shot examples to use in each test sample\n",
    "    task=task,\n",
    "    prefix=None,                        # Answer prefix for the evaluation. Appended to prompt after the question and before the answer\n",
    "    system_prompt=None,                 # System prompt for the evaluation. If None, no system prompt is used\n",
    "    add_generation_prompt=True,         # Whether to add generation prompt when using chat template\n",
    ")\n",
    "contrastive_dataset = constructor.construct_dataset()\n",
    "evaluation_dataset = eval_constructor.construct_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e32ca98",
   "metadata": {},
   "source": [
    "### Evaluation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6507563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
      "<|im_start|>user\n",
      "An astronomer observes that a planet rotates faster after a meteorite impact. Which is the most likely effect of this increase in rotation?\n",
      "\n",
      "Choices:\n",
      "A: Planetary density will decrease.\n",
      "B: Planetary years will become longer.\n",
      "C: Planetary days will become shorter.\n",
      "D: Planetary gravity will become stronger.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(evaluation_dataset[0]['input'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e6a330",
   "metadata": {},
   "source": [
    "### Contrastive pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48b9afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to view the first contrastive sample pair\n",
    "\n",
    "print(contrastive_dataset[0]['positive'])\n",
    "print(\"=\"*50)\n",
    "print(contrastive_dataset[0]['negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb5e288",
   "metadata": {},
   "source": [
    "## Extracting and applying vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d6f94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exctract the steering vectors for each layer\n",
    "# The function returns a dict {\"values\": {layer_idx: steering_vector}, \"keys\": {layer_idx: steering_vector}}\n",
    "steering_kv = extract_steering_kv(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=contrastive_dataset,\n",
    "    steering_config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7371f61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(evaluation_dataset[0]['input'], return_tensors=\"pt\").to(device)\n",
    "\n",
    "steering_config = SteeringConfig(\n",
    "    tokenizer=tokenizer,\n",
    "    c_keys=0.0,                     # The steering coefficient for the keys        \n",
    "    c_values=3,                     # The steering coefficient for the values\n",
    "    append_special_token=True,      # Whether to append a special token to the input to offset the position of the steering token. Allows the alignment of the extraction and application tokens.\n",
    ")\n",
    "\n",
    "generation_kwargs = {\"max_new_tokens\": 100, \"do_sample\": False}\n",
    "output = generate_with_cache_steering(\n",
    "    model,\n",
    "    tokens[\"input_ids\"],\n",
    "    steering_kv=steering_kv,\n",
    "    steering_config=steering_config,\n",
    "    attention_mask=tokens[\"attention_mask\"],\n",
    "    **generation_kwargs,\n",
    ")\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1da897f",
   "metadata": {},
   "source": [
    "## Running experiments with Evaluator class\n",
    "\n",
    "The `Evaluator` class accepts the evaluation and contrastive datasets togeher with steering config as arguments, extracts steering vectors, and applies cache steering to evaluation. This class was written for the convenience of running experiments. However, you don't have to use this class to use cache steering as it compatible with pure pytorch + HuggingFace transformers (see section below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eeefe5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 14:20:06,469 - helpers.py - generate_vector_id - INFO - Generated vector ID: 6e9928a1-19c1-50b2-b373-3979b92e19da\n",
      "2025-07-11 14:20:06,470 - evaluator.py - extract_steering_vectors - INFO - Loading steering vector from cache: 6e9928a1-19c1-50b2-b373-3979b92e19da\n",
      "2025-07-11 14:20:06,471 - helpers.py - load_vector - INFO - Loading vector from cached_vectors/6e9928a1-19c1-50b2-b373-3979b92e19da.pt\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 367.73 examples/s]\n",
      "Pre-tokenizing: 2it [00:00, 880.69it/s]\n",
      "Generating responses: 100%|██████████| 2/2 [00:11<00:00,  5.92s/it]\n"
     ]
    }
   ],
   "source": [
    "steering_config = SteeringConfig(\n",
    "    tokenizer=tokenizer,\n",
    "    how='last',                     # The position to apply steering to. Passing 'last' would appply steering to the token. Passing an integer i would apply steering to token position -i\n",
    "    c_keys=0.0,                     # The steering coefficient for the keys        \n",
    "    c_values=2,                     # The steering coefficient for the values\n",
    "    layers_ids_keys=[1],            # The layers to apply steering to for the keys. If None, steering is applied to all layers except the embedding layer. If [i] is passed, steering is applied to layer i and above. If [i, j] is passed, steering is applied to layers i and j only.\n",
    "    layers_ids_values=[1],          # Same as layers_ids_keys, but for the values\n",
    "    append_special_token=True,      # Whether to append a special token to the input to offset the position of the steering token. Allows the alignment of the extraction and application tokens.\n",
    ")\n",
    "steering_config.set_seed(42)        # Set seed for vector id generation (doesn't affect the steering process itself, this is a coding artefact that wasn't implemented properly). The Evaluator will cache the steering vectors with the same parameters, so changing the seed is a way to recompute the steering vector without removing the old one from the cache\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 1\n",
    "\n",
    "evaluator = Evaluator(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    evaluation_dataset,\n",
    "    task,\n",
    "    device=device,\n",
    "    steering_config=steering_config,\n",
    "    extraction_dataset=contrastive_dataset,\n",
    ")\n",
    "generation_kwargs = {\"max_new_tokens\": 512, \"do_sample\": False}\n",
    "\n",
    "results = evaluator.evaluate(batch_size, generation_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc895d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
      "<|im_start|>user\n",
      "A group of engineers wanted to know how different building designs would respond during an earthquake. They made several models of buildings and tested each for its ability to withstand earthquake conditions. Which will most likely result from testing different building designs?\n",
      "\n",
      "Choices:\n",
      "A: buildings will be built faster\n",
      "B: buildings will be made safer\n",
      "C: building designs will look nicer\n",
      "D: building materials will be cheaper<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "A: Buildings will be built faster\n",
      "\n",
      "The engineers tested different building designs to determine which would be most effective in resisting earthquake forces. The most likely outcome is that the building designs that were tested would be built faster, as they would be designed to be more efficient and cost-effective. This would allow the engineers to complete the testing and analysis more quickly, potentially saving time and resources.\n",
      "So the correct choice is A: buildings will be built faster.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "generated_sample = results['samples'][0]\n",
    "print(generated_sample['input'])\n",
    "print(generated_sample['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92afa73",
   "metadata": {},
   "source": [
    "### Try the same input without steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97749af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
      "<|im_start|>user\n",
      "A group of engineers wanted to know how different building designs would respond during an earthquake. They made several models of buildings and tested each for its ability to withstand earthquake conditions. Which will most likely result from testing different building designs?\n",
      "\n",
      "Choices:\n",
      "A: buildings will be built faster\n",
      "B: buildings will be made safer\n",
      "C: building designs will look nicer\n",
      "D: building materials will be cheaper<|im_end|>\n",
      "<|im_start|>assistant\n",
      "A: buildings will be built faster<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer(generated_sample['input'], return_tensors='pt')\n",
    "output_tokens = model.generate(**tokens, **generation_kwargs)\n",
    "print(tokenizer.decode(output_tokens[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006c3c9e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea58fd89",
   "metadata": {},
   "source": [
    "# Implementing cache steering with PyTorch + Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ea857d",
   "metadata": {},
   "source": [
    "## Vector Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad7eba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import DynamicCache\n",
    "\n",
    "\n",
    "def extract_steering_kv(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    data,\n",
    "    batch_size=1,\n",
    "    device=\"cpu\",\n",
    "    extraction_token_position: int = -1,  # Position of the token to extract the steering vectors from, -1 means the last token\n",
    "):\n",
    "    steering_values = defaultdict(lambda: torch.tensor([]).to(device))\n",
    "    steering_keys = defaultdict(lambda: torch.tensor([]).to(device))\n",
    "\n",
    "    for example in tqdm(data.iter(batch_size=batch_size)):\n",
    "\n",
    "        pos_tokens = tokenizer(example['positive'], return_tensors='pt', padding=True).to(device)\n",
    "        neg_tokens = tokenizer(example['negative'], return_tensors='pt', padding=True).to(device)\n",
    "\n",
    "        # Find indices of the tokens to extract the steering vectors from\n",
    "        pos_indices = pos_tokens['attention_mask'].sum(dim=1) + extraction_token_position\n",
    "        neg_indices = neg_tokens['attention_mask'].sum(dim=1) + extraction_token_position\n",
    "        batch_indices = torch.arange(pos_tokens['input_ids'].size(0), device=pos_tokens['input_ids'].device)\n",
    "\n",
    "        # Record cache for the positive and negative tokens\n",
    "        cache_positive, cache_negative = DynamicCache(), DynamicCache()\n",
    "        with torch.no_grad():\n",
    "            _ = model(**pos_tokens, output_hidden_states=True, past_key_values=cache_positive)\n",
    "            _ = model(**neg_tokens, output_hidden_states=True, past_key_values=cache_negative)\n",
    "\n",
    "        for layer_id in range(len(cache_positive.value_cache)):\n",
    "            pos_values = cache_positive.value_cache[layer_id][batch_indices, :, pos_indices, :]\n",
    "            neg_values = cache_negative.value_cache[layer_id][batch_indices, :, neg_indices, :]\n",
    "\n",
    "            pos_keys = cache_positive.key_cache[layer_id][batch_indices, :, pos_indices, :]\n",
    "            neg_keys = cache_negative.key_cache[layer_id][batch_indices, :, neg_indices, :]\n",
    "\n",
    "            # Take the differnece between the vectors\n",
    "            steering_values[layer_id] = torch.cat([steering_values[layer_id], pos_values - neg_values]) # [batch_size, n_heads, head_dim]\n",
    "            steering_keys[layer_id] = torch.cat([steering_keys[layer_id], pos_keys - neg_keys])\n",
    "\n",
    "    # Average the vectors\n",
    "    for layer_id in steering_values:\n",
    "        steering_values[layer_id] = torch.mean(steering_values[layer_id], dim=0) # [n_heads, head_dim]\n",
    "        steering_keys[layer_id] = torch.mean(steering_keys[layer_id], dim=0)\n",
    "\n",
    "    return {\n",
    "        \"values\": steering_values,\n",
    "        \"keys\": steering_keys,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efb857c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['positive', 'negative'],\n",
      "    num_rows: 10\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:24,  2.44s/it]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "# Prepare the contrastive dataset for steering vector extraction\n",
    "positive_examples = contrastive_dataset['positive'] # or any variable of type List[text]\n",
    "negative_examples = contrastive_dataset['negative'] # or any variable of type List[text]\n",
    "contrastive_set = Dataset.from_dict({\n",
    "    \"positive\": positive_examples,\n",
    "    \"negative\": negative_examples,\n",
    "})\n",
    "print(contrastive_set)\n",
    "\n",
    "# Extract the steering vectors from the contrastive dataset\n",
    "steering_vectors = extract_steering_kv(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    contrastive_set,\n",
    "    batch_size=batch_size,\n",
    "    device=device,\n",
    "    extraction_token_position=-1,  # Last token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b9fb1ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check if the steering vectors the same as in the evaluator\n",
    "torch.all(\n",
    "    torch.isclose(steering_vectors[\"values\"][1], evaluator.steering_kv[\"values\"][1])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6509966f",
   "metadata": {},
   "source": [
    "## Generate with Cache Steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46782bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from transformers import PreTrainedModel, BatchEncoding\n",
    "\n",
    "\n",
    "def generate_with_cache_steering(\n",
    "    model: PreTrainedModel,\n",
    "    tokens: BatchEncoding | torch.Tensor,\n",
    "    steering_kv: dict,\n",
    "    application_token_idx: int = -1,\n",
    "    offset_str: Optional[str] = None,\n",
    "    c_keys: float = 0.0,\n",
    "    c_values: float = 0.0,\n",
    "    **kwargs,\n",
    "):\n",
    "    # Check the format of tokens and convert if necessary\n",
    "    if isinstance(tokens, BatchEncoding):\n",
    "        tokens = tokens[\"input_ids\"]\n",
    "\n",
    "    # Append a special token to the input tokens if you need to steer the cache of last token and want to align with the extraction token\n",
    "    if offset_str and application_token_idx == -1:\n",
    "        token_to_append = tokenizer(offset_str, add_special_tokens=False)[\"input_ids\"][0]\n",
    "        token_to_append = torch.ones(tokens.shape[0], 1, device=tokens.device, dtype=tokens.dtype) * token_to_append\n",
    "        tokens = torch.cat([tokens, token_to_append], dim=-1)\n",
    "        if \"attention_mask\" in kwargs:\n",
    "            kwargs['attention_mask'] = torch.cat([kwargs['attention_mask'], torch.ones_like(token_to_append)], dim=-1)\n",
    "\n",
    "    # Create the initial cache\n",
    "    cache_input = {\n",
    "        \"input_ids\": tokens,\n",
    "        \"attention_mask\": kwargs['attention_mask']\n",
    "    }\n",
    "    past_key_values = precompute_kv_cache(model, cache_input)\n",
    "\n",
    "    # Steer the cache\n",
    "    past_key_values = steer_kv_cache(\n",
    "        past_key_values,\n",
    "        steering_kv,\n",
    "        application_token_idx=application_token_idx,\n",
    "        c_keys=c_keys,\n",
    "        c_values=c_values,\n",
    "    )\n",
    "\n",
    "    # Generate as usual\n",
    "    output = model.generate(\n",
    "        tokens,\n",
    "        past_key_values=past_key_values,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return output\n",
    "\n",
    "\n",
    "def precompute_kv_cache(model, tokens):\n",
    "    \"\"\"\n",
    "    Precompute the key and value caches for the input tokens except the last one.\n",
    "    \"\"\"\n",
    "    past_key_values = DynamicCache()\n",
    "\n",
    "    if isinstance(tokens, BatchEncoding) or isinstance(tokens, dict):\n",
    "        cache_input = {\n",
    "            k: v[:, :-1]\n",
    "            for k, v in tokens.items()\n",
    "            if k in [\"input_ids\", \"attention_mask\", \"token_type_ids\", \"position_ids\"]\n",
    "        }\n",
    "    else:\n",
    "        cache_input = {\"input_ids\": tokens[:, :-1]}\n",
    "\n",
    "    # Compute correct position_ids before caching\n",
    "    seq_lengths = cache_input[\"attention_mask\"].sum(dim=1)\n",
    "    position_ids = torch.zeros_like(cache_input[\"input_ids\"])\n",
    "    for i in range(cache_input[\"input_ids\"].shape[0]):\n",
    "        valid_len = seq_lengths[i]\n",
    "        position_ids[i, -valid_len:] = torch.arange(valid_len)\n",
    "    cache_input[\"position_ids\"] = position_ids\n",
    "\n",
    "    # Precompute the KV cache\n",
    "    with torch.no_grad():\n",
    "        model(**cache_input, past_key_values=past_key_values, use_cache=True)\n",
    "\n",
    "    return past_key_values\n",
    "\n",
    "\n",
    "def steer_kv_cache(cache, steering_kv, application_token_idx=-1, c_keys=0.0, c_values=0.0):\n",
    "\n",
    "    # Steer the values cache\n",
    "    if \"values\" in steering_kv:\n",
    "        for layer_idx, past_values in steering_kv[\"values\"].items():\n",
    "            steer_kv_cache_layer(\n",
    "                cache,\n",
    "                past_values,\n",
    "                layer_idx,\n",
    "                type=\"values\",\n",
    "                application_token_idx=application_token_idx,\n",
    "                c_keys=c_keys,\n",
    "                c_values=c_values,\n",
    "            )\n",
    "\n",
    "    # Steer the keys cache\n",
    "    if \"keys\" in steering_kv:\n",
    "        for layer_idx, past_keys in steering_kv[\"keys\"].items():\n",
    "            steer_kv_cache_layer(\n",
    "                cache,\n",
    "                past_keys,\n",
    "                layer_idx,\n",
    "                type=\"keys\",\n",
    "                application_token_idx=application_token_idx,\n",
    "                c_keys=c_keys,\n",
    "                c_values=c_values,\n",
    "            )\n",
    "\n",
    "    return cache\n",
    "\n",
    "\n",
    "def steer_kv_cache_layer(\n",
    "    cache,\n",
    "    steering_vector,\n",
    "    layer_idx,\n",
    "    type=\"values\",\n",
    "    application_token_idx=-1,\n",
    "    c_keys=0.0,\n",
    "    c_values=0.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Steer the key and value cache of a specific layer.\n",
    "    \"\"\"\n",
    "    # Clone the steering vector to avoid modifying the original dict\n",
    "    sv = steering_vector.clone() # [n_heads, head_dim]\n",
    "\n",
    "    # Apply the vector to the cache\n",
    "    if type == 'values':\n",
    "        cache.value_cache[layer_idx][:, :, application_token_idx, :] += sv * c_values\n",
    "\n",
    "    elif type == 'keys':\n",
    "        cache.key_cache[layer_idx][:, :, application_token_idx, :] += sv * c_keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fca15704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last last token in the prompt: '\\n'\n",
      "Last token in contrastive dataset examples: '\\n'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize a prompt and prepare it for generation\n",
    "prompt = \"What is the capital of France?\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "input_tokens = tokenizer(input_text, return_tensors='pt')\n",
    "\n",
    "# Let's inspect the last token\n",
    "application_last_token_id = input_tokens['input_ids'][0, -1].item()\n",
    "extraction_last_token_id = tokenizer(positive_examples[0])['input_ids'][-1]\n",
    "print(f\"Last last token in the prompt: {repr(tokenizer.decode(application_last_token_id))}\")\n",
    "print(f\"Last token in contrastive dataset examples: {repr(tokenizer.decode(extraction_last_token_id))}\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598d1cd3",
   "metadata": {},
   "source": [
    "Here you can see that we extracted our vectors from `\\n` and the last token in the input prompt is also `\\n`. Through experimentation we found that such tokens as `\\n`, `.`, tokenizer template tokens, BoS, EoS, etc. aggregate the most information. Therefore, we might want to offset the size of the cache to be able to add the vectors extracted from `\\n` in the contrastive set, to vectors corresponding to `\\n` in the KV cache of the target prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bdd513",
   "metadata": {},
   "source": [
    "Let's first validate if the implementation is identical to normal `model.generate()` if the coefficients are 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a3cb8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original output: <|im_start|>system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
      "<|im_start|>user\n",
      "What is the capital of France?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The capital of France is Paris.<|im_end|>\n",
      "\n",
      " ================================================== \n",
      "\n",
      "Output with empty steering: <|im_start|>system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
      "<|im_start|>user\n",
      "What is the capital of France?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The capital of France is Paris.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# Let's validate if the implementation is identical to normal model.generate() if the coefficients are 0\n",
    "genreration_kwargs = {\"max_new_tokens\": 128, \"do_sample\": False}\n",
    "original_output = model.generate(\n",
    "    input_tokens['input_ids'],\n",
    "    attention_mask=input_tokens['attention_mask'],\n",
    "    **genreration_kwargs,\n",
    ")\n",
    "print(\"Original output:\", tokenizer.decode(original_output[0], skip_special_tokens=False))\n",
    "\n",
    "empty_steering_output = generate_with_cache_steering(\n",
    "    model,\n",
    "    input_tokens['input_ids'],\n",
    "    attention_mask=input_tokens['attention_mask'],\n",
    "    steering_kv=steering_vectors,\n",
    "    **genreration_kwargs,\n",
    ")\n",
    "print(\"\\n\", \"=\" * 50, \"\\n\")\n",
    "print(\"Output with empty steering:\", tokenizer.decode(empty_steering_output[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79512b05",
   "metadata": {},
   "source": [
    "Now let's steer the cache with the KV steering vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b013c155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== \n",
      "\n",
      "Steered output: <|im_start|>system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
      "<|im_start|>user\n",
      "What is the capital of France?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "When it comes to the capital of France, it is essential to note that the official capital of France is Paris, which is also the largest city in the country. Paris is not only the capital but also the economic, cultural, and political center of France.\n",
      "\n",
      "In terms of geography, Paris is located in the northern part of France, in the region of Nord-Pas-de-Calais. It is situated on the Seine River, which flows through the city, and is surrounded by several other major cities, including Lyon, Marseille, and Reims.\n",
      "\n",
      "In terms of population, Paris has\n"
     ]
    }
   ],
   "source": [
    "# The real KV steering\n",
    "steered_output = generate_with_cache_steering(\n",
    "    model,\n",
    "    input_tokens['input_ids'],\n",
    "    attention_mask=input_tokens['attention_mask'],\n",
    "    steering_kv=steering_vectors,\n",
    "    application_token_idx=-1,   # Last token\n",
    "    offset_str=\"\\n\",            # Some token (in string format) to append to the input to offset the position of the steered token\n",
    "    c_keys=0.0,\n",
    "    c_values=10.0,\n",
    "    **genreration_kwargs,\n",
    ")\n",
    "print(\"=\" * 50, \"\\n\")\n",
    "print(\"Steered output:\", tokenizer.decode(steered_output[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18bfcabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== \n",
      "\n",
      "Steered output: <|im_start|>system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
      "<|im_start|>user\n",
      "What is the capital of France?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "The capital of France is Paris.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# To confirm the effect is not attributed to appending '\\n' to the input :)\n",
    "steered_output = generate_with_cache_steering(\n",
    "    model,\n",
    "    input_tokens['input_ids'],\n",
    "    attention_mask=input_tokens['attention_mask'],\n",
    "    steering_kv=steering_vectors,\n",
    "    offset_str=\"\\n\",\n",
    "    c_keys=0.0,\n",
    "    c_values=0.0,\n",
    "    **genreration_kwargs,\n",
    ")\n",
    "print(\"=\" * 50, \"\\n\")\n",
    "print(\"Steered output:\", tokenizer.decode(steered_output[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab8a121",
   "metadata": {},
   "source": [
    "### Style transfer example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d0cddc",
   "metadata": {},
   "source": [
    "Let's try to use cache steering to induce the `analogical_reasoning` style. The contrastive examples from this subset follow this pattern: `Just like [some analogy], ...`, so we expect our model to produce something similar if steering is effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3259f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocss ARC-challenge dataset\n",
    "task = Tasks.arc_oai\n",
    "dataset = load_task_dataset(Tasks.arc_oai, subtask=\"analogical_reasoning\")\n",
    "\n",
    "config = SteeringConfig(\n",
    "    n_contrastive_samples=20,  \n",
    "    num_fewshot_examples=5,\n",
    "    tokenizer=tokenizer,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "constructor = ContrastiveDatasetConstructor(\n",
    "    dataset[\"train\"],\n",
    "    config,\n",
    "    task=task,\n",
    ")\n",
    "contrastive_dataset = constructor.construct_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e050a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the contrastive dataset for steering vector extraction\n",
    "positive_examples = contrastive_dataset['positive']\n",
    "negative_examples = contrastive_dataset['negative']\n",
    "contrastive_set = Dataset.from_dict({\n",
    "    \"positive\": positive_examples,\n",
    "    \"negative\": negative_examples,\n",
    "})\n",
    "\n",
    "# Extract the steering vectors from the contrastive dataset\n",
    "steering_vectors = extract_steering_kv(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    contrastive_set,\n",
    "    batch_size=1,\n",
    "    device=device,\n",
    "    extraction_token_position=-1,  # Last token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65f2ab62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== \n",
      "\n",
      "Steered output: <|im_start|>system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
      "<|im_start|>user\n",
      "What is the capital of France?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "Just like how you have a home, a country has a capital. The capital of France is Paris. Paris is a big city in France, and it's where the Eiffel Tower is located. It's a beautiful city with lots of history and culture.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# Tokenize a prompt and prepare it for generation\n",
    "prompt = \"What is the capital of France?\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "input_tokens = tokenizer(input_text, return_tensors='pt')\n",
    "\n",
    "# Generate\n",
    "genreration_kwargs = {\"max_new_tokens\": 128, \"do_sample\": False}\n",
    "steered_output = generate_with_cache_steering(\n",
    "    model,\n",
    "    input_tokens['input_ids'],\n",
    "    attention_mask=input_tokens['attention_mask'],\n",
    "    steering_kv=steering_vectors,\n",
    "    application_token_idx=-1,\n",
    "    offset_str=\"\\n\",\n",
    "    c_keys=0.0,\n",
    "    c_values=2.0,\n",
    "    **genreration_kwargs,\n",
    ")\n",
    "print(\"=\" * 50, \"\\n\")\n",
    "print(\"Steered output:\", tokenizer.decode(steered_output[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8121da66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
